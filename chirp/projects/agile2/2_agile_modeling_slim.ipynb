{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to the agile modelling Python notebook. \n",
    "\n",
    "## What is a Python notebook?\n",
    "\n",
    "A Python notebook allows you to run Python code in a Python environment. If you are running this notebook in Google Colab, the Python notebook is running in a virtual machine in the cloud. \n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will use a process called \"[agile modeling](https://arxiv.org/abs/2302.12948)\" build and incrementally improve a classifier for acoustic analysis, starting from a single classified example. This process uses embeddings provided by the Perch model. \n",
    "These are the steps we will take:\n",
    "\n",
    "1. Installing and importing Perch and other requirements\n",
    "2. Configuring the Perch agile modelling modules\n",
    "3. Creating a database of embeddings\n",
    "4. Searchin for recordings similar to the single provided example\n",
    "5. Building a machine learning classifier model from the search results\n",
    "6. Searching your recordings based on the results of the classifier\n",
    "7. Improving your classifier further using these search results\n",
    "\n",
    "This steps of this agile modeling process are described more visually in these [slides](https://docs.google.com/presentation/d/e/2PACX-1vTfvoBvCi_V72s0RiIcmFNdnZDcPDCDl-omBbODJ3sz3_IxD5kd1zJjd-J8AR7PE_DgxO-FWDjyP7Mb/pub?start=false&loop=false&delayms=3000&slide=id.g2d63d0c2ccf_0_3915)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing and importing Perch and dependencies\n",
    "\n",
    "You are running this notebook in a Python environment. We need to add the Perch package to this environment. We do this by running the `pip install` command below. You only need to do this once, however if you are running this notebook in the cloud on Google Colab, your session is only ephemaral. You need to rerun his this cell after disconnecting. \n",
    "\n",
    "You will need to restart the session after this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/QutEcoacoustics/perch.git@6193a00f04703f8bb959cb6250719a7ef7580049 python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXQAcreKedWU"
   },
   "outputs": [],
   "source": [
    "#@title Imports. { vertical-output: true }\n",
    "from pathlib import Path\n",
    "from chirp.projects.agile2.agile_modeling_state import agile2_config, agile2_state, download_embeddings, Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking to Google Drive\n",
    "\n",
    "We will need somewhere to read and write files. This colab environment where the notebook is running does not persist between sessions, so we will link to google drive for access to persistent storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "except:\n",
    "    print(\"colab not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Here we set some configuration for names and local filepaths and initialize our agile modeling workflow.\n",
    "\n",
    "Your Ecosounds \"auth_token\" can be found by logging in to https://www.ecosounds.org, then clicking on your profile picture in the top left. You can copy your auth token from this profile page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "config = agile2_config()\n",
    "\n",
    "config.search_dataset_name=\"search_dataset\" #@param {type:'string'}\n",
    "config.annotator_id=\"phil\" #@param {type:'string'}\n",
    "\n",
    "# If you followed the above instructions for creating a shortcut to the Drive folder, \n",
    "# you should be able to navigate to this directory in the left hand \"Files\" menu \n",
    "# in this Colab (indicated by the Folder icon on the far left menu).\n",
    "\n",
    "base_folder = '/content/drive/My Drive/' \n",
    "\n",
    "# This is the location on google drive that this tutorial will use to save data.\n",
    "working_folder = Path(base_folder) / 'esa2024_data/'\n",
    "\n",
    "config.db_path = Path(working_folder) / 'db/db.sqlite'\n",
    "config.embeddings_folder = Path(working_folder) / 'embeddings/'\n",
    "config.labeled_examples_folder = Path(working_folder) / 'labeled_examples/'\n",
    "config.models_folder = Path(working_folder) / 'models/'\n",
    "config.predictions_folder = Path(working_folder) / 'predictions/'\n",
    "\n",
    "Path(config.labeled_examples_folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "agile = agile2_state(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring access to your Ecosounds project\n",
    "\n",
    "We will be loading audio from [Ecosounds](https://www.ecosounds.org), an online repository of ecoacoustic recordings. If working with a private Ecosounds project, we need to provide the Ecosounds *auth token* associated with your ecosounds account. \n",
    "\n",
    "**Note: if you are using one of the demo sets, you can skip this step**\n",
    "\n",
    "To find your ecosounds token, go to https://www.ecosounds.org/my_account and in the bottom left, click on the button to copy the token. \n",
    "\n",
    "Because this is a secret, we should avoid saving it in plain text in the notebook, so we will set it up in an environment variable. Depending on where you are running this notebook, do one of the following (if you are not sure, just run the cell below and it will tell you).\n",
    "\n",
    "If working in **colab**, do the following:\n",
    "1. On the right, click on the key icon to open the \"secrets\" tab.\n",
    "2. Click \"Add new secret\"\n",
    "3. Under \"Name\" put the text `BAW_AUTH_TOKEN` (without quotes)\n",
    "4. Under \"Value\" paste the token you copied from Ecosounds\n",
    "\n",
    "If working in **jupyter running locally**, do the following\n",
    "1. In the working directory create a file named `.env`\n",
    "   - The working directory is probably the directory where you launched the notebook. If you are not sure, run the cell below and it will tell you. \n",
    "2. In this .env file, put the line `BAW_AUTH_TOKEN=abc123xyz` (replace the abc123xyz with the token you copied from Ecosounds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "auth_token = None\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    auth_token = userdata.get('BAW_AUTH_TOKEN')\n",
    "    print(\"Got auth token from colab secrets\")\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    env_file = find_dotenv()\n",
    "    if not env_file:\n",
    "        print(f\"No .env file found in the working directory {os.getcwd()}. \\nFollow the local Jupyter instructions above to create one.\")\n",
    "    else:\n",
    "        load_dotenv(override=True)\n",
    "        auth_token = os.getenv('BAW_AUTH_TOKEN')\n",
    "        if auth_token:\n",
    "            print(f\"Got auth token from .env file {env_file}\")\n",
    "        else:\n",
    "            print(\"BAW_AUTH_TOKEN env variable not found in your .env file. Follow the local Jupyter instructions above to set it\")\n",
    "    \n",
    "except userdata.SecretNotFoundError:\n",
    "        print(\"No BAW_AUTH_TOKEN secret found, please follow the colab instructions above to set it\")\n",
    "\n",
    "if auth_token:\n",
    "    if config.baw_config.get('auth_token'):\n",
    "        print(\"Overwriting config auth token with new value\")\n",
    "    config.baw_config['auth_token'] = auth_token\n",
    "elif config.baw_config.get('auth_token'):\n",
    "    print(\"Auth token not loaded, but already in config\")\n",
    "else:\n",
    "    print(\"No auth token set\")\n",
    "\n",
    "\n",
    "config.baw_config['domain'] = 'api.ecosounds.org'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embeddings database\n",
    "\n",
    "What is an embedding? \n",
    "\n",
    "Digital audio (a long sequence of sound pressure measurements) is in a form is difficult for machine learning to use for call recognition. This is because it contains a lot of irrelevant and redundant information. Deep learning algorithms can figure out which parts of the information contained in the raw audio is relevant for discriminating between animal call types by looking millions of examples from tens of thousands of species. It learns a large function that converts the audio into a much smaller representation that contains only this relevant information, known as an *embedding*. We can then apply this function, known as the *embedding model* to any audio to obtain embeddings.Unlike the raw audio signal or specrogram image, this embedding representation can be used by machine learning to do call-recognition.\n",
    "\n",
    "In this step we are retrieving file of embeddings for each of the recordings that we will be searching in, and put them in the right format for working with them. \n",
    "\n",
    "The `search_set_id` is the id of your search set, preprepared on Ecosounds. \n",
    "\n",
    "If you have uploaded audio for this workshop, you will have been given an id for that search set. \n",
    "\n",
    "You can also use one of the following public search sets\n",
    "\n",
    "1. `yellow_bellied_glider`\n",
    "2. `powerful_owl`\n",
    "3. `powerful_owl_subset` (a smaller dataset that takes less time to prepreocess and search through)\n",
    "4. `forty_spotted_pardalote`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download audio embeddings to the working folder\n",
    "# this might take a while\n",
    "\n",
    "download_embeddings(config.search_dataset_name, config.embeddings_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the downloaded embeddings, create a database of embeddings.\n",
    "# This database links labels to embeddings so we can train our classifier\n",
    "# this might take a while\n",
    "agile.create_database(config.embeddings_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agile.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVdLJJd9gnjo"
   },
   "source": [
    "# Search\n",
    "\n",
    "Here, we take a single example and find the examples in our search set which most closely match that example. This is a way to get started with a labelled training set.\n",
    "\n",
    "If you have short examples of your target call, copy them into the labeled_examples_folder and then run the following cell to check that they are accessible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your labelled examples in a folder on your mounted Google Drive, \n",
    "# Run this cell to list the audio files in that folder\n",
    "\n",
    "audio_files = Helpers.list_audio_files(config.labeled_examples_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load a query audio. You can enter the item number for one of the examples listed in the previous cell output. Or alternatively you can enter a a URL, filepath, or Xeno-Canto ID in the form `xc777802`.\n",
    "\n",
    "This will display the example and allow you to select the 5-second portion of it to use. Note: if your example is too long, this can make the selection of the 5-second segment a bit more difficult. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ig3L5dsy3mr"
   },
   "outputs": [],
   "source": [
    "#@title Load query audio. { vertical-output: true }\n",
    "\n",
    "\n",
    "#@markdown The `query` can be an item number in your labeled examples folder, or\n",
    "#@markdown  a URL, filepath, or Xeno-Canto ID\n",
    "#@markdown (like `xc777802`, containing an Eastern Whipbird (`easwhi1`)).\n",
    "query = '0' # @param {type:'string'}\n",
    "\n",
    "agile.display_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate an embedding for the 5-second example and then compare it against the embeddings in the database to find similar 5-second clips from your search dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHUJ_NwQWZNB"
   },
   "outputs": [],
   "source": [
    "\n",
    "#@markdown Number of results to retrieve.\n",
    "num_results = 10  #@param\n",
    "#@markdown Number of (randomly selected) database entries to search over.\n",
    "sample_size = None  #@param\n",
    "#@markdown When margin sampling, target this logit.\n",
    "target_score = None  #@param\n",
    "\n",
    "agile.embed_query()\n",
    "\n",
    "agile.search_with_query(\n",
    "        num_results=num_results,\n",
    "        sample_size=sample_size,\n",
    "        target_score=target_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, look at some of these search results. For each example you can look at a spectrogram and listen to the audio, then apply a positive label if it is the target class, and negative if it's not the target class.  Set the `query_label` variable to define the label that this button will add to these embeddings in your database. \n",
    "\n",
    "Click the label once to turn it green for a positive label, and twice to turn it orange for a negative label. Leave it unclicked if you don't want to apply any label to the example (i.e. don't add it to your labelled training set). \n",
    "\n",
    "Note: loading the spectrograms can sometimes fail. If you see some examples that failed to load, try running the cell a second time before starting your labelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Our target call-type label\n",
    "query_label = 'ybg'  #@param {type:'string'}\n",
    "\n",
    "agile.display_search_results(query_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have finished labelling, run the following cell to save these newly labelled examples to the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3sIkOqlXzKB"
   },
   "outputs": [],
   "source": [
    "#@title Save data labels. { vertical-output: true }\n",
    "\n",
    "agile.save_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the above steps with a few different audio queries. If your target species has multiple call types, it would be a good idea to search for at least one of each call type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o65wpjvyYft-"
   },
   "source": [
    "# Classify\n",
    "\n",
    "Now that we have labelled a number of our embedded audio clips in the search set, we have what we need to train and evaluate a classifier. \n",
    "\n",
    "This classifier is a statistical model. It is trained through an algorithm called Gradient Descent. This process requires setting some parameters.\n",
    "\n",
    "For the most part, you the default values for all of these will work fine. You might want to start by leaving the defaults, then experiment later to see if you can get any improvements. \n",
    "\n",
    "1. **Target Labels**: If you have put more than one class into your embeddings database, and you don't want to build the model to include all of these, list the ones you do want to include\n",
    "\n",
    "The following are hyperparameters of the gradient descent algorithm. The algorithm looks at 'batches' of examples and updates the model's parameters a little after each batch. \n",
    "\n",
    "2. **learning_rate**: How much to update by after each batch\n",
    "3. **batch_size**: How many examples to include in each batch (generally leave it pretty high and only reduce if you have RAM problems)\n",
    "4. **num_steps**: How many times to look at all the batches. When you train, you will see the \"loss\" decreasing as it learns. The num_steps should ideally be set so that arrives at the last step just after it stops improving. \n",
    "\n",
    "The following are to do with the labelled data inputs. \n",
    "\n",
    "5. **train_ratio**: A random subset of the labelled audio is not used to train the model, but instead is used to test the model. This is so we know roughly how well the model does on classifying examples that it has never seen before. \n",
    "3. **weak_neg_weight**: In your database we have a lot of audio, most of which is probably not your target. By taking some random clips from your unlabelled audio and treating them as negative examples, we can train on a wider variety of negative examples than what has been explicitly labelled as negative. However, because we don't know for sure that this process didn't choose a positive example by chance, we give each one less importance in the training. \n",
    "6. **weak_negatives_batch_size**: How many of these randomly chosen examples to include for each batch (on top of the number in the strongly labelled batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtsJkgcPYg6z"
   },
   "outputs": [],
   "source": [
    "#@title Classifier training. { vertical-output: true }\n",
    "\n",
    "#@markdown Set of labels to classify. If None, auto-populated from the DB.\n",
    "target_labels = None  #@param\n",
    "\n",
    "learning_rate = 1e-3  #@param\n",
    "num_steps = 101  #@param\n",
    "batch_size = 32  #@param\n",
    "\n",
    "train_ratio = 0.9  #@param\n",
    "weak_neg_weight = 0.05  #@param\n",
    "weak_negatives_batch_size = 16  #@param\n",
    "\n",
    "agile.train_classifier(target_labels, learning_rate, weak_neg_weight, num_steps, train_ratio, batch_size, weak_negatives_batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained classifier, we can follow the same process as for the single example query. We search the database for more examples using the classifier, label them, then re-train the classifier with the new examples. \n",
    "\n",
    "The classifier outputs a score for each example in the search set. It believes anything above zero belongs to your target class, and anything below doesn't. \n",
    "\n",
    "When searching using the classifier, we can look for examples with the highest score by setting `target_score` to `None`.  This might get us more positive examples but these probably won't improve the classifier much, because they already have a high score. More useful is to search for those examples that the classifier is least sure about, by setting `target_score` to `0`.  Try setting the target score to None, 0 and possibly some other values depending on how many positive examples come back from each of those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Review Classifier Results. { vertical-output: true }\n",
    "#@markdown Our target call-type label\n",
    "query_label = 'ybg'  #@param {type:'string'}\n",
    "#@markdown Number of results to retrieve.\n",
    "num_results = 10  #@param\n",
    "#@markdown Number of (randomly selected) database entries to search over.\n",
    "sample_size = None  #@param\n",
    "#@markdown When margin sampling, target this logit.\n",
    "target_score = None  #@param\n",
    "\n",
    "agile.search_with_classifier(query_label, num_results, sample_size, target_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3N6dzhetkG1"
   },
   "outputs": [],
   "source": [
    "agile.display_search_results(query_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEk15jw_B8xL"
   },
   "outputs": [],
   "source": [
    "#@title Save data labels. { vertical-output: true }\n",
    "\n",
    "agile.save_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving your classifier and running inference\n",
    "\n",
    "The trained classifier consists of the following elements\n",
    "1. The *weights* of the model (how to multiply and add the embedding values together to produce higher scores for the examples of the target class than other examples), which were learned during training. \n",
    "2. The model *bias* (how to shift the scores so that scores for positive examples are positive and vice-versa), also learned during training. \n",
    "3. The list of labels (class names) corresponding to the output scores\n",
    "4. Some metadata related to the model that created the embeddings, so that if the classifier is used on new audio, we make sure to embed in a compatible way. \n",
    "\n",
    "With this information, the classifier can be saved and used on other search sets later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = 'ybg_01'  #@param {type:'string'}\n",
    "\n",
    "classifier_path = Path(config.models_folder) / f'{classifier_name}.json'\n",
    "\n",
    "classifier_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "agile.classifier.save(classifier_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run the model over all of the search dataset and save the results to a csv. Specify:\n",
    "1. The csv filename to save the results to\n",
    "2. The threshold. Anything above the threshold for the target labels will be saved. Typically this would be zero to save anything the classifier believes is the target class\n",
    "3. Which labels to include. Leave it as None to include all the labels you trained for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = 'ybg_output.csv'  #@param {type:'string'}\n",
    "\n",
    "threshold=0.0 #@param {type:'string'}\n",
    "\n",
    "# You can also specify a random subset of the dataset to run inference on, e.g. 0.5 for 50%\n",
    "subset = 0.1 #@param {type:'string'}\n",
    "\n",
    "# Which labels to include in the output file. If None, all labels are included.\n",
    "labels = None\n",
    "\n",
    "output_filepath = Path(agile.config.predictions_folder) / output_filename\n",
    "Path(output_filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "agile.run_inference(output_filepath, threshold=0.0, dataset=config.search_dataset_name, subset=subset)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
